# Import des librairies nécessaires
```{r}
library(tidyverse)
library(compositions)
library(factoextra)
library(FactoMineR)
```

# Import du dataset
```{r}
df <- read.csv("dataset.csv")

```

# Résumé du dataset
```{r}
str(df)
summary(df)
```

# Analyse univariée
```{r}
df %>%
  select_if(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 30) +
  facet_wrap(~ key, scales = 'free') +
  theme_minimal()
```
Ce qu'on observe via ces premiers graphique c'est que nous avons principalement des variables quantitatives. 

Certaines de ces variables sont compositionnelles et d'autres représentent des taux. Parmis les variables compositionnelles on retrouve les catégories socio-professionnelles qui une fois additionnés par région nous donne 100% ce qui représente la population dans son entiereté.

# Analyse bivariée 
```{r}
# Matrice de corrélation
correlation_matrix <- cor(df %>% select_if(is.numeric))
corrplot::corrplot(correlation_matrix, method = "circle")
```
```{r}
# Variables compositionnelles
compositionnal_vars <- c("HLM", "Ouvrier", "Employe", "Cadres", "Artisant", "Agri")

# Transformation CLR sur les données compositionnelles
clr_data <- clr(df[, compositionnal_vars])

# Sélectionner les autres variables d'intérêt
other_vars <- c("Salairemoy", "TxPauv", "NonDiplome", "txcho", "txabs")  # Assurez-vous de bien écrire "txabs"
other_data <- df[, other_vars]

# Combiner les données CLR et les autres variables dans un même dataframe
df_correlation <- cbind(as.data.frame(clr_data), other_data)

# Calculer la matrice de corrélation
correlation_matrix <- cor(df_correlation)

# Visualiser la matrice de corrélation
library(corrplot)
corrplot(correlation_matrix, method = "circle")
```
La corrélation est plus importante une fois la transformation des valeurs composite effectuée, mais seulement sur certaines variables. Globalement on a une variation positive et négative au niveau des différentes corrélations. 

```{r}
# Scatter plots pour les variables numériques
ggplot(df, aes(x = TxPauv, y = txabs)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal() +
  labs(title = "Relation entre le taux de pauvreté et le taux d'abstention")

```


# Variables descriptives
```{r}
# Nombre de variables descriptives
descriptive_vars <- df %>% select_if(is.numeric) %>% colnames()
length(descriptive_vars)  # Nombre de variables

# données compositionnelles
compositionnal_vars <- c("HLM", "Ouvrier", "Employe", "Cadres", "Artisant", "Agri")
```

```{r}
# Effectuer une transformation CLR sur les données compositionnelles
clr_data <- clr(df[, compositionnal_vars])

# Effectuer une PCA sur les données transformées
pca_result <- prcomp(clr_data, scale. = TRUE)
summary(pca_result)

# Visualiser les résultats PCA
biplot(pca_result)
```
PC1 est la composante la plus importante, expliquant une grande partie de la variance dans vos données.
PC2 et PC3 sont également significatifs mais expliquent beaucoup moins de variance.

```{r}
# Supprimer les colonnes non pertinentes
df_clean <- df[, !colnames(df) %in% c("X", "Department", "Code")]
```

```{r}
# Variables compositionnelles
compositionnal_vars <- c("HLM", "Ouvrier", "Employe", "Cadres", "Artisant", "Agri")

# Effectuer une transformation CLR sur les données compositionnelles
clr_data <- clr(df[, compositionnal_vars])

# Créer un nouveau dataframe pour l'ACP, en combinant clr_data avec d'autres variables si nécessaire
df_clr <- df_clean  # Copie de df_clean
df_clr[, compositionnal_vars] <- clr_data  # Remplace les colonnes compositionnelles par clr_data

# Première ACP avec CLR
result_acp_2 <- PCA(df_clr, scale.unit = TRUE, ncp = 5, graph = FALSE)
fviz_screeplot(result_acp_2)
fviz_pca_var(result_acp_2)
fviz_pca_ind(result_acp_2)

# Deuxième ACP sans CLR
data_acp <- scale(df_clean[sapply(df_clean, is.numeric)])
result_acp <- PCA(df_clean, scale.unit = TRUE, ncp = 5, graph = FALSE)
fviz_screeplot(result_acp)
fviz_pca_var(result_acp)
fviz_pca_ind(result_acp)
```
La différence entre la PCA avec et sans la transformation des donnée est assez significative. On remarque par exemple que les variables HLP, PI et Cadres on une forte corrélation avec l'axe 1 et la variable employe avec l'axe 2 alors que sans la tranformation des données les variables qui ont une forte corrélation avec l'axe 2 sont Taux de pauvreté, taux d'absetention et taux de chomage. Pour l'axe 1 il s'agissait de Agriculteur et Non Diplomé.

Là ou il y a une grande différence c'est au niveau du % d'explication obtenu via la prmière dimension. La première dimension explique 34% de la variance lorsqu'on en transforme pas les données et elle explique 42% de la variance lors que les données sont transformées et traitées correctement.

Sur 2 dimensions on est donc à 34.8 + 23.9; soit 58.7% environ de variance expliquée par les 2 premières dimensions. Suite au traitement des variables cette valeur passe à 42.1 + 22.5; soit 64.6%.


```{r}
acm_data <- df %>%
  select(Ouvrier, Employe, PI, Cadres, Artisant, Agri)

str(acm_data)

acm_data<-acm_data %>%
  mutate(across(everything(), as.factor))

acm_result<-MCA(acm_data, graph = FALSE)

summary(acm_result)
```

# Commenter le résultat

```{r}
# Liste des colonnes
variables <- colnames(acm_data)

# Initialiser une liste pour stocker les résultats
results <- list()

# Boucle sur toutes les paires de variables
for (i in 1:(length(variables) - 1)) {
  for (j in (i + 1):length(variables)) {
    var1 <- variables[i]
    var2 <- variables[j]
    table_contingence <- table(acm_data[[var1]], acm_data[[var2]])
    
    # Effectuer le test du Chi-Deux
    test <- chisq.test(table_contingence)
    
    # Sauvegarder le résultat
    results[[paste(var1, var2, sep = " vs ")]] <- list(
      p_value = test$p.value,
      statistic = test$statistic
    )
  }
}

# Afficher les résultats
print(results)

```

# Commenter le résultat

```{r}

```

